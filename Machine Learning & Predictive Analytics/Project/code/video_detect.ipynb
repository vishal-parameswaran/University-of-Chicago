{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import cvlib as cv\n",
    "from PIL import Image, ImageFont, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "    \n",
    "    # return the resized image\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(face_crop,face_gray,size=100):\n",
    "    w,h = face_crop.shape[1],face_crop.shape[0]\n",
    "    new_image = np.zeros((size,size,3))\n",
    "    new_gray_image = np.zeros((48,48))\n",
    "    if w>h:\n",
    "        image = image_resize(face_crop,width=size)\n",
    "        image_gray = image_resize(face_gray,width=48)\n",
    "        height_offset = int((new_image.shape[0] - image.shape[0])/2)\n",
    "        new_image[height_offset:int(image.shape[0]+height_offset), :image.shape[1]] = image\n",
    "    elif h>w:\n",
    "        image = image_resize(face_crop,height=size)\n",
    "        image_gray = image_resize(face_gray,height=48)\n",
    "        width_offset = int((new_image.shape[1] - image.shape[1])/2)\n",
    "        new_image[:image.shape[0], width_offset:int(image.shape[1] + width_offset)] = image\n",
    "    else:\n",
    "        image = image_resize(face_crop,width=size,height=size)\n",
    "        image_gray = image_resize(face_gray,width=48,height=48)\n",
    "        new_image[:image.shape[0], :image.shape[1]] = image\n",
    "    new_gray_image[:image_gray.shape[0], :image_gray.shape[1]] = image_gray\n",
    "    new_gray_image = np.expand_dims(new_gray_image,axis=2)\n",
    "    cv2.imshow(\"\",new_image)\n",
    "    face_crop = new_image.astype(\"float\") / 255.0\n",
    "    face_gray = new_gray_image.astype(\"float\")/255.0\n",
    "    return face_crop,face_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "bmi_net = load_model(\"best_model_vgg_complex_100.h5\")\n",
    "emotion_classifier = load_model(\"sentiment_standard.h5\")\n",
    "class_map = {0: 'angry',1: 'disgust',2: 'fear',3: 'happy',4: 'neutral',5: 'sad',6: 'surprise'}\n",
    "print(\"[INFO] starting video stream...\")\n",
    "webcam = cv2.VideoCapture(0)\n",
    "font = ImageFont.truetype(\"../data/Uni Sans Heavy.otf\", 12)\n",
    "# loop over the frames from the video stream\n",
    "while webcam.isOpened():\n",
    "    status, frame = webcam.read()\n",
    "    face, confidence = cv.detect_face(frame)\n",
    "    new_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = []\n",
    "    gray_faces = []\n",
    "    bboxes = []\n",
    "    preds = []\n",
    "    for idx, f in enumerate(face):\n",
    "        print(f)\n",
    "        face_crop = []\n",
    "        face_gray = []\n",
    "        (startX, startY) = f[0], f[1]\n",
    "        (endX, endY) = f[2], f[3]\n",
    "        \n",
    "        cv2.rectangle(frame, (startX,startY), (endX,endY), (6,214,160), 1)\n",
    "        startX = startX - 100\n",
    "        startY = startY -100\n",
    "        endX = endX + 100\n",
    "        endY = endY + 100\n",
    "        startX = 0 if startX<1 else startX\n",
    "        startY = 0 if startY<1 else startY\n",
    "        if endX - startX < 50:\n",
    "            continue \n",
    "        face_crop = np.copy(frame[startY:endY,startX:endX])\n",
    "        face_gray = np.copy(gray[startY:endY,startX:endX])\n",
    "        face_crop,face_gray = pre_process_image(face_crop,face_gray,size=100)\n",
    "        \n",
    "        faces.append(face_crop)\n",
    "        gray_faces.append(face_gray)\n",
    "        Y = f[1] - 10 if f[1] - 10 > 10 else f[1] + 10\n",
    "        bboxes.append((f[0],Y))\n",
    "    im = Image.fromarray(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    if len(faces)>0:\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        gray_faces = np.array(gray_faces, dtype=\"float32\")\n",
    "        conf = bmi_net.predict(faces,batch_size=32)\n",
    "        emotion = emotion_classifier.predict(gray_faces,batch_size=32)\n",
    "        emotions = [(class_map[sub_emotion.argmax()],max(sub_emotion)) for sub_emotion in emotion]\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        for location,pred,emotion in zip(bboxes,conf,emotions):\n",
    "            conf_text = \"BMI : \" + str(int(pred))\n",
    "            emotion_text = \"Emotion: \" + str(emotion[0]) + \"-\" + str(emotion[1])\n",
    "            draw.text((location[0], location[1]-30), conf_text,(6,214,160),font=font)\n",
    "            draw.text((location[0], location[1]-15), emotion_text,(6,214,160),font=font)\n",
    "    frame = cv2.cvtColor(np.array(im),cv2.COLOR_RGB2BGR)\n",
    "    # display output\n",
    "    cv2.imshow(\"BMI Detection\", frame)\n",
    "    # im.show()\n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        webcam.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
